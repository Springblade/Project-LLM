{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30ee374-d48b-4f69-887c-024ff8a134ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2026.1.3: Fast Qwen2_Vl patching. Transformers: 4.57.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 5090. Num GPUs = 1. Max memory: 31.367 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.11.0.dev20260119+cu128. CUDA: 12.0. CUDA Toolkit: 12.8. Triton: 3.6.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Qwen2-VL 2B Base model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastVisionModel\n",
    "import torch\n",
    "from transformers import AutoProcessor\n",
    "\n",
    "# 1. Load Model (Remove max_pixels here)\n",
    "model, tokenizer = FastVisionModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen2-VL-7B-Instruct-bnb-4bit\",\n",
    "    load_in_4bit = True,\n",
    "    max_seq_length = 32768,\n",
    ")\n",
    "\n",
    "# 2. Re-load the processor with pixel limits if needed\n",
    "# (The 'tokenizer' returned by Unsloth is actually the Processor for vision models)\n",
    "# You can also set these during inference, but setting them here is cleaner.\n",
    "min_pixels = 256 * 28 * 28\n",
    "max_pixels = 512 * 512 # or 1280 * 28 * 28 for higher quality\n",
    "\n",
    "# Set to inference mode\n",
    "FastVisionModel.for_inference(model)\n",
    "\n",
    "print(\"Qwen2-VL 7B Base model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99434a61-0cc4-4f08-a18b-935e1f731992",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]/workspace/unsloth_env/lib/python3.12/site-packages/bitsandbytes/_ops.py:239: FutureWarning: _check_is_size will be removed in a future PyTorch release along with guard_size_oblivious.     Use _check(i >= 0) instead.\n",
      "  torch._check_is_size(blocksize)\n",
      "/workspace/unsloth_env/lib/python3.12/site-packages/bitsandbytes/_ops.py:186: FutureWarning: _check_is_size will be removed in a future PyTorch release along with guard_size_oblivious.     Use _check(i >= 0) instead.\n",
      "  torch._check_is_size(blocksize)\n",
      "/workspace/unsloth_env/lib/python3.12/site-packages/bitsandbytes/_ops.py:239: FutureWarning: _check_is_size will be removed in a future PyTorch release along with guard_size_oblivious.     Use _check(i >= 0) instead.\n",
      "  torch._check_is_size(blocksize)\n",
      "/workspace/unsloth_env/lib/python3.12/site-packages/bitsandbytes/_ops.py:186: FutureWarning: _check_is_size will be removed in a future PyTorch release along with guard_size_oblivious.     Use _check(i >= 0) instead.\n",
      "  torch._check_is_size(blocksize)\n",
      "/workspace/unsloth_env/lib/python3.12/site-packages/bitsandbytes/_ops.py:239: FutureWarning: _check_is_size will be removed in a future PyTorch release along with guard_size_oblivious.     Use _check(i >= 0) instead.\n",
      "  torch._check_is_size(blocksize)\n",
      "/workspace/unsloth_env/lib/python3.12/site-packages/bitsandbytes/_ops.py:186: FutureWarning: _check_is_size will be removed in a future PyTorch release along with guard_size_oblivious.     Use _check(i >= 0) instead.\n",
      "  torch._check_is_size(blocksize)\n",
      "/workspace/unsloth_env/lib/python3.12/site-packages/bitsandbytes/_ops.py:239: FutureWarning: _check_is_size will be removed in a future PyTorch release along with guard_size_oblivious.     Use _check(i >= 0) instead.\n",
      "  torch._check_is_size(blocksize)\n",
      "/workspace/unsloth_env/lib/python3.12/site-packages/bitsandbytes/_ops.py:186: FutureWarning: _check_is_size will be removed in a future PyTorch release along with guard_size_oblivious.     Use _check(i >= 0) instead.\n",
      "  torch._check_is_size(blocksize)\n",
      "/workspace/unsloth_env/lib/python3.12/site-packages/bitsandbytes/_ops.py:239: FutureWarning: _check_is_size will be removed in a future PyTorch release along with guard_size_oblivious.     Use _check(i >= 0) instead.\n",
      "  torch._check_is_size(blocksize)\n",
      "/workspace/unsloth_env/lib/python3.12/site-packages/bitsandbytes/_ops.py:186: FutureWarning: _check_is_size will be removed in a future PyTorch release along with guard_size_oblivious.     Use _check(i >= 0) instead.\n",
      "  torch._check_is_size(blocksize)\n",
      "/workspace/unsloth_env/lib/python3.12/site-packages/bitsandbytes/_ops.py:239: FutureWarning: _check_is_size will be removed in a future PyTorch release along with guard_size_oblivious.     Use _check(i >= 0) instead.\n",
      "  torch._check_is_size(blocksize)\n",
      "/workspace/unsloth_env/lib/python3.12/site-packages/bitsandbytes/_ops.py:186: FutureWarning: _check_is_size will be removed in a future PyTorch release along with guard_size_oblivious.     Use _check(i >= 0) instead.\n",
      "  torch._check_is_size(blocksize)\n",
      "/workspace/unsloth_env/lib/python3.12/site-packages/bitsandbytes/backends/cuda/ops.py:464: FutureWarning: _check_is_size will be removed in a future PyTorch release along with guard_size_oblivious.     Use _check(i >= 0) instead.\n",
      "  torch._check_is_size(blocksize)\n",
      "/workspace/unsloth_env/lib/python3.12/site-packages/bitsandbytes/backends/cuda/ops.py:464: FutureWarning: _check_is_size will be removed in a future PyTorch release along with guard_size_oblivious.     Use _check(i >= 0) instead.\n",
      "  torch._check_is_size(blocksize)\n",
      "/workspace/unsloth_env/lib/python3.12/site-packages/bitsandbytes/_ops.py:239: FutureWarning: _check_is_size will be removed in a future PyTorch release along with guard_size_oblivious.     Use _check(i >= 0) instead.\n",
      "  torch._check_is_size(blocksize)\n",
      "/workspace/unsloth_env/lib/python3.12/site-packages/bitsandbytes/_ops.py:284: FutureWarning: _check_is_size will be removed in a future PyTorch release along with guard_size_oblivious.     Use _check(i >= 0) instead.\n",
      "  torch._check_is_size(blocksize)\n",
      "/workspace/unsloth_env/lib/python3.12/site-packages/bitsandbytes/_ops.py:239: FutureWarning: _check_is_size will be removed in a future PyTorch release along with guard_size_oblivious.     Use _check(i >= 0) instead.\n",
      "  torch._check_is_size(blocksize)\n",
      "/workspace/unsloth_env/lib/python3.12/site-packages/bitsandbytes/_ops.py:284: FutureWarning: _check_is_size will be removed in a future PyTorch release along with guard_size_oblivious.     Use _check(i >= 0) instead.\n",
      "  torch._check_is_size(blocksize)\n",
      "/workspace/unsloth_env/lib/python3.12/site-packages/bitsandbytes/_ops.py:239: FutureWarning: _check_is_size will be removed in a future PyTorch release along with guard_size_oblivious.     Use _check(i >= 0) instead.\n",
      "  torch._check_is_size(blocksize)\n",
      "/workspace/unsloth_env/lib/python3.12/site-packages/bitsandbytes/_ops.py:284: FutureWarning: _check_is_size will be removed in a future PyTorch release along with guard_size_oblivious.     Use _check(i >= 0) instead.\n",
      "  torch._check_is_size(blocksize)\n",
      "/workspace/unsloth_env/lib/python3.12/site-packages/bitsandbytes/_ops.py:239: FutureWarning: _check_is_size will be removed in a future PyTorch release along with guard_size_oblivious.     Use _check(i >= 0) instead.\n",
      "  torch._check_is_size(blocksize)\n",
      "/workspace/unsloth_env/lib/python3.12/site-packages/bitsandbytes/_ops.py:284: FutureWarning: _check_is_size will be removed in a future PyTorch release along with guard_size_oblivious.     Use _check(i >= 0) instead.\n",
      "  torch._check_is_size(blocksize)\n",
      "/workspace/unsloth_env/lib/python3.12/site-packages/bitsandbytes/backends/cuda/ops.py:464: FutureWarning: _check_is_size will be removed in a future PyTorch release along with guard_size_oblivious.     Use _check(i >= 0) instead.\n",
      "  torch._check_is_size(blocksize)\n",
      "100%|██████████| 16/16 [02:10<00:00,  8.16s/it]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Update the output filename to distinguish it from your fine-tuned results\n",
    "INPUT_FILE = \"Diagnose_dataset.json\"\n",
    "OUTPUT_FILE = \"BaseModel_predictions_output.json\"\n",
    "\n",
    "def download_image(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        return Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "with open(INPUT_FILE, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for key, item in tqdm(data.items()):\n",
    "    if item.get(\"base_model_answer\"): continue\n",
    "\n",
    "    instruction = item[\"question\"][\"Text\"] + \" What would be the most likely disease diagnosis(es) for this patient?\"\n",
    "    image_urls = eval(item[\"question\"][\"ImageList\"]) \n",
    "    \n",
    "    images = [download_image(url) for url in image_urls]\n",
    "    images = [img for img in images if img is not None]\n",
    "\n",
    "    if not images: continue\n",
    "\n",
    "    # Qwen2-VL prompt format\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                *([{\"type\": \"image\"} for _ in range(len(images))]),\n",
    "                {\"type\": \"text\", \"text\": instruction},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    # Use the processor (tokenizer) to handle the images correctly\n",
    "    input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        images = images,\n",
    "        text = input_text,\n",
    "        add_special_tokens = False,\n",
    "        return_tensors = \"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens = 512,\n",
    "            use_cache = True,\n",
    "            temperature = 0.2,\n",
    "            top_p = 0.9,\n",
    "            # Ensure the model knows when to stop\n",
    "            pad_token_id = tokenizer.tokenizer.pad_token_id,\n",
    "            eos_token_id = tokenizer.tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Qwen-specific cleaning\n",
    "    final_answer = prediction.split(\"assistant\\n\")[-1].strip()\n",
    "    item[\"base_model_answer\"] = final_answer\n",
    "\n",
    "    if int(key) % 10 == 0:\n",
    "        with open(OUTPUT_FILE, 'w') as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "\n",
    "with open(OUTPUT_FILE, 'w') as f:\n",
    "    json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7f731c-1267-4222-96e3-3e65350de031",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Unsloth (RTX 5090)",
   "language": "python",
   "name": "unsloth_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
