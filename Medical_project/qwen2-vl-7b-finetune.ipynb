{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14100975,"sourceType":"datasetVersion","datasetId":8980445},{"sourceId":14501991,"sourceType":"datasetVersion","datasetId":9262370},{"sourceId":14559996,"sourceType":"datasetVersion","datasetId":9299882}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\nimport os\nos.environ[\"TORCHDYNAMO_DISABLE\"] = \"1\"\nos.environ[\"UNSLOTH_DISABLE_DYNAMO\"] = \"1\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T16:19:28.381144Z","iopub.execute_input":"2026-01-20T16:19:28.381734Z","iopub.status.idle":"2026-01-20T16:19:28.385993Z","shell.execute_reply.started":"2026-01-20T16:19:28.381708Z","shell.execute_reply":"2026-01-20T16:19:28.385217Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%capture\n!pip install --no-deps \\\n    bitsandbytes accelerate peft trl triton cut_cross_entropy unsloth_zoo\n\n!pip install \\\n    sentencepiece protobuf \\\n    \"datasets==4.3.0\" \\\n    \"huggingface_hub>=0.34.0\" \\\n    hf_transfer\n\n!pip install --no-deps unsloth\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T16:19:35.337163Z","iopub.execute_input":"2026-01-20T16:19:35.337780Z","iopub.status.idle":"2026-01-20T16:19:51.136358Z","shell.execute_reply.started":"2026-01-20T16:19:35.337757Z","shell.execute_reply":"2026-01-20T16:19:51.135312Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%capture\n!pip install transformers==4.56.2\n!pip install --no-deps trl==0.22.2\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T16:20:18.122027Z","iopub.execute_input":"2026-01-20T16:20:18.123080Z","iopub.status.idle":"2026-01-20T16:20:23.039296Z","shell.execute_reply.started":"2026-01-20T16:20:18.123042Z","shell.execute_reply":"2026-01-20T16:20:23.038459Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch, unsloth\nprint(\"torch:\", torch.__version__)\nprint(\"cuda:\", torch.cuda.is_available())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T16:20:23.040810Z","iopub.execute_input":"2026-01-20T16:20:23.041194Z","iopub.status.idle":"2026-01-20T16:21:01.315940Z","shell.execute_reply.started":"2026-01-20T16:20:23.041165Z","shell.execute_reply":"2026-01-20T16:21:01.315252Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LOAD MODEL","metadata":{}},{"cell_type":"code","source":"from unsloth import FastVisionModel\nimport torch\nmodel, tokenizer = FastVisionModel.from_pretrained(\n    \"unsloth/Qwen2-VL-7B-Instruct-unsloth-bnb-4bit\",\n    load_in_4bit = True, # Use 4bit to reduce memory use. False for 16bit LoRA.\n    max_seq_length = 2048,   # Context length - can be longer, but uses more memory\n    load_in_8bit = False,    # A bit more accurate, uses 2x memory\n    full_finetuning = False, # We have full finetuning now!\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T16:21:01.317219Z","iopub.execute_input":"2026-01-20T16:21:01.317480Z","iopub.status.idle":"2026-01-20T16:21:48.689717Z","shell.execute_reply.started":"2026-01-20T16:21:01.317462Z","shell.execute_reply":"2026-01-20T16:21:48.688909Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = FastVisionModel.get_peft_model(\n    model,\n    finetune_vision_layers     = True, # False if not finetuning vision layers\n    finetune_language_layers   = True, # False if not finetuning language layers\n    finetune_attention_modules = True, # False if not finetuning attention layers\n    finetune_mlp_modules       = True, # False if not finetuning MLP layers\n\n    r = 16,           # The larger, the higher the accuracy, but might overfit\n    lora_alpha = 16,  # Recommended alpha == r at least\n    lora_dropout = 0,\n    bias = \"none\",\n    random_state = 3407,\n    use_rslora = False,  # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n    # target_modules = \"all-linear\", # Optional now! Can specify a list if needed\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T16:22:10.991922Z","iopub.execute_input":"2026-01-20T16:22:10.992446Z","iopub.status.idle":"2026-01-20T16:22:11.089771Z","shell.execute_reply.started":"2026-01-20T16:22:10.992423Z","shell.execute_reply":"2026-01-20T16:22:11.088772Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LOAD DATASET ","metadata":{}},{"cell_type":"code","source":"import json\n\nwith open(\"/kaggle/input/trieuds/train_multimodal.json\", \"r\", encoding=\"utf-8\") as f:\n    raw = json.load(f)\n\ncleaned = []\n\nfor sample in raw:\n    user = sample[\"messages\"][0]\n    assistant = sample[\"messages\"][1]\n\n    # Extract user text\n    user_text_parts = []\n    user_image_paths = []\n\n    for item in user[\"content\"]:\n        if item[\"type\"] == \"text\":\n            user_text_parts.append(item[\"text\"])\n        elif item[\"type\"] == \"image\":\n            user_image_paths.append(item[\"image_path\"])\n\n    cleaned.append({\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": None,\n                \"texts\": user_text_parts,\n                \"image_paths\": user_image_paths if user_image_paths else None\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": assistant[\"content\"],\n                \"texts\": None,\n                \"image_paths\": None\n            }\n        ]\n    })\n\nwith open(\"clean_dataset.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(cleaned, f, indent=2, ensure_ascii=False)\n\nprint(\"DONE â€” cleaned_dataset.json generated.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T16:22:21.692216Z","iopub.execute_input":"2026-01-20T16:22:21.692811Z","iopub.status.idle":"2026-01-20T16:22:21.728165Z","shell.execute_reply.started":"2026-01-20T16:22:21.692787Z","shell.execute_reply":"2026-01-20T16:22:21.727587Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\nwith open(\"clean_dataset.json\", \"r\") as f:\n    train_dataset3 = json.load(f)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T16:22:30.688591Z","iopub.execute_input":"2026-01-20T16:22:30.689432Z","iopub.status.idle":"2026-01-20T16:22:30.696653Z","shell.execute_reply.started":"2026-01-20T16:22:30.689406Z","shell.execute_reply":"2026-01-20T16:22:30.695882Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DEFINE FORMATTING FUNCTION","metadata":{}},{"cell_type":"code","source":"train_dataset3[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T16:22:34.967021Z","iopub.execute_input":"2026-01-20T16:22:34.967863Z","iopub.status.idle":"2026-01-20T16:22:34.972455Z","shell.execute_reply.started":"2026-01-20T16:22:34.967835Z","shell.execute_reply":"2026-01-20T16:22:34.971877Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def to_chatml(sample):\n    \"\"\"\n    TRL-safe ChatML formatter.\n    Handles:\n      - str\n      - dict (single sample)\n      - LazyBatch (batched samples)\n    Returns: List[str]\n    \"\"\"\n\n    # Case 1: already formatted text\n    if isinstance(sample, str):\n        return [sample]\n\n    # Case 2: batched input (LazyBatch)\n    if \"messages\" in sample and isinstance(sample[\"messages\"], list) and \\\n       len(sample[\"messages\"]) > 0 and isinstance(sample[\"messages\"][0], list):\n        outputs = []\n        for messages in sample[\"messages\"]:\n            outputs.extend(to_chatml({\"messages\": messages}))\n        return outputs\n\n    # Case 3: single dict sample\n    conversation = []\n\n    for msg in sample[\"messages\"]:\n        role = msg[\"role\"]\n        conversation.append(f\"<|{role}|>\")\n\n        if msg.get(\"image_paths\"):\n            for img in msg[\"image_paths\"]:\n                conversation.append(f\"<image>{img}</image>\")\n\n        if msg.get(\"texts\"):\n            for text in msg[\"texts\"]:\n                conversation.append(text)\n\n        if msg.get(\"content\"):\n            conversation.append(msg[\"content\"])\n\n    return [\"\\n\".join(conversation)]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T16:22:39.551311Z","iopub.execute_input":"2026-01-20T16:22:39.551553Z","iopub.status.idle":"2026-01-20T16:22:39.557959Z","shell.execute_reply.started":"2026-01-20T16:22:39.551536Z","shell.execute_reply":"2026-01-20T16:22:39.557162Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(type(train_dataset3[0]))\nprint(to_chatml(train_dataset3[0]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T16:22:42.358179Z","iopub.execute_input":"2026-01-20T16:22:42.358873Z","iopub.status.idle":"2026-01-20T16:22:42.363091Z","shell.execute_reply.started":"2026-01-20T16:22:42.358846Z","shell.execute_reply":"2026-01-20T16:22:42.362268Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import Dataset\n\ntrain_dataset3 = Dataset.from_list(train_dataset3)\ntrain_dataset3 = train_dataset3.shuffle(seed = 3407)\ntrain_dataset3[0]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T16:22:55.054008Z","iopub.execute_input":"2026-01-20T16:22:55.054625Z","iopub.status.idle":"2026-01-20T16:22:55.114156Z","shell.execute_reply.started":"2026-01-20T16:22:55.054587Z","shell.execute_reply":"2026-01-20T16:22:55.113559Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# TRAINER & TRAINING","metadata":{}},{"cell_type":"code","source":"from trl import SFTTrainer, SFTConfig\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=train_dataset3,\n    eval_dataset=None,\n    args=SFTConfig(\n        dataset_text_field=None,   \n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=8,\n        warmup_steps=5,\n        num_train_epochs=3,\n        max_steps=200,\n        learning_rate=1e-5,\n        logging_steps=1,\n        optim=\"adamw_8bit\",\n        weight_decay=0.001,\n        lr_scheduler_type=\"linear\",\n        seed=3407,\n        report_to=\"none\",\n    ),\n    formatting_func=to_chatml\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T16:39:12.943448Z","iopub.execute_input":"2026-01-20T16:39:12.943844Z","iopub.status.idle":"2026-01-20T16:39:16.642751Z","shell.execute_reply.started":"2026-01-20T16:39:12.943822Z","shell.execute_reply":"2026-01-20T16:39:16.642151Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# @title Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T16:39:18.679313Z","iopub.execute_input":"2026-01-20T16:39:18.679835Z","iopub.status.idle":"2026-01-20T16:39:18.685681Z","shell.execute_reply.started":"2026-01-20T16:39:18.679805Z","shell.execute_reply":"2026-01-20T16:39:18.684682Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T16:39:20.479951Z","iopub.execute_input":"2026-01-20T16:39:20.480227Z","iopub.status.idle":"2026-01-20T17:48:09.715982Z","shell.execute_reply.started":"2026-01-20T16:39:20.480208Z","shell.execute_reply":"2026-01-20T17:48:09.715371Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# @title Show final memory and time stats\nused_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory / max_memory * 100, 3)\nlora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\nprint(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\nprint(\n    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n)\nprint(f\"Peak reserved memory = {used_memory} GB.\")\nprint(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\nprint(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\nprint(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T17:50:13.671374Z","iopub.execute_input":"2026-01-20T17:50:13.671792Z","iopub.status.idle":"2026-01-20T17:50:13.677861Z","shell.execute_reply.started":"2026-01-20T17:50:13.671771Z","shell.execute_reply":"2026-01-20T17:50:13.677123Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# TEST","metadata":{}},{"cell_type":"code","source":"#messages = [\n#    {\"role\" : \"user\", \"content\" : \"Solve (x + 2)^2 = 0.\"}\n#]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize = False,\n    add_generation_prompt = True, # Must add for generation\n    enable_thinking = False, # Disable thinking\n)\n\nfrom transformers import TextStreamer\n_ = model.generate(\n    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n    max_new_tokens = 256, # Increase for longer outputs!\n    temperature = 0.7, top_p = 0.8, top_k = 20, # For non thinking\n    streamer = TextStreamer(tokenizer, skip_prompt = True),\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# SAVE MODEL TO DOWNLOAD","metadata":{}},{"cell_type":"code","source":"# Save the model\nmodel.save_pretrained(\"qwen2_7b\")\ntokenizer.save_pretrained(\"qwen2_7b\")\n\n# Create a zip file to download\nimport zipfile\nimport os\n\n# Create zip file of the model\nwith zipfile.ZipFile('qwen2_7b.zip', 'w') as zipf:\n    for root, dirs, files in os.walk('qwen2_7b'):\n        for file in files:\n            zipf.write(os.path.join(root, file))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T17:51:43.912234Z","iopub.execute_input":"2026-01-20T17:51:43.912503Z","iopub.status.idle":"2026-01-20T17:51:46.038047Z","shell.execute_reply.started":"2026-01-20T17:51:43.912485Z","shell.execute_reply":"2026-01-20T17:51:46.037232Z"}},"outputs":[],"execution_count":null}]}