{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8f28ebe-c09b-4685-b0b6-7eb804bd8339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/unsloth_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2026.1.4: Fast Qwen2_Vl patching. Transformers: 4.57.6.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 5090. Num GPUs = 1. Max memory: 31.367 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 12.0. CUDA Toolkit: 12.8. Triton: 3.6.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.34. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastVisionModel\n",
    "import torch\n",
    "model, tokenizer = FastVisionModel.from_pretrained(\n",
    "    \"unsloth/Qwen2-VL-7B-Instruct-unsloth-bnb-4bit\",\n",
    "    load_in_4bit = True, # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
    "    max_seq_length = 2048,   # Context length - can be longer, but uses more memory\n",
    "    load_in_8bit = False,    # A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False, # We have full finetuning now!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f42b173c-aaf1-44fc-bbc7-cc406359ae27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model.visual` require gradients\n"
     ]
    }
   ],
   "source": [
    "model = FastVisionModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = True, # False if not finetuning vision layers\n",
    "    finetune_language_layers   = True, # False if not finetuning language layers\n",
    "    finetune_attention_modules = True, # False if not finetuning attention layers\n",
    "    finetune_mlp_modules       = True, # False if not finetuning MLP layers\n",
    "\n",
    "    r = 8,           # The larger, the higher the accuracy, but might overfit\n",
    "    lora_alpha = 16,  # Recommended alpha == r at least\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    "    # target_modules = \"all-linear\", # Optional now! Can specify a list if needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ba17818-2471-4d3f-872e-8e7977d78425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dataset_format(input_data):\n",
    "    new_dataset = []\n",
    "\n",
    "    for item in input_data:\n",
    "        new_messages = []\n",
    "        image_path = None\n",
    "\n",
    "        for msg in item[\"messages\"]:\n",
    "            role = msg[\"role\"]\n",
    "            content = msg[\"content\"]\n",
    "\n",
    "            text_parts = []\n",
    "\n",
    "            if isinstance(content, list):\n",
    "                for part in content:\n",
    "                    if part[\"type\"] == \"image\":\n",
    "                        image_path = part[\"image_path\"]\n",
    "                    elif part[\"type\"] == \"text\":\n",
    "                        text_parts.append(part[\"text\"])\n",
    "                final_text = \" \".join(text_parts)\n",
    "            else:\n",
    "                final_text = content\n",
    "\n",
    "            new_messages.append({\n",
    "                \"role\": role,\n",
    "                \"content\": final_text\n",
    "            })\n",
    "\n",
    "        new_entry = {\n",
    "            \"messages\": new_messages,\n",
    "            \"images\": [image_path] if image_path else []\n",
    "        }\n",
    "\n",
    "        new_dataset.append(new_entry)\n",
    "\n",
    "    return new_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79a73d13-492d-4414-bdef-a8a9c9ee5c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"workspace/Dataset/train_multimodal.json\", \"r\") as f:\n",
    "    raw_data = json.load(f)\n",
    "    raw_data = convert_dataset_format(raw_data)\n",
    "def normalize_sample(sample):\n",
    "    for msg in sample[\"messages\"]:\n",
    "        if isinstance(msg[\"content\"], str):\n",
    "            msg[\"content\"] = [{\"type\": \"text\", \"text\": msg[\"content\"]}]\n",
    "    return sample\n",
    "\n",
    "\n",
    "normalized = [normalize_sample(x) for x in raw_data]\n",
    "\n",
    "with open(\"workspace/Dataset/course_dataset_normalized.json\", \"w\") as f:\n",
    "    json.dump(normalized, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25480b30-50ec-460c-a2ab-2640cd6c6e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [00:00<00:00, 28452.32 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from datasets import Dataset\n",
    "\n",
    "base_path = \"workspace/Dataset\"\n",
    "\n",
    "def extract_text(content):\n",
    "    if isinstance(content, str):\n",
    "        return content\n",
    "    if isinstance(content, list):\n",
    "        for block in content:\n",
    "            if block.get(\"type\") == \"text\":\n",
    "                return block.get(\"text\", \"\")\n",
    "    return \"\"\n",
    "\n",
    "def to_unsloth_vl(examples):\n",
    "    messages_out = []\n",
    "\n",
    "    for i in range(len(examples[\"messages\"])):\n",
    "        msgs = examples[\"messages\"][i]\n",
    "        images = examples[\"images\"][i]\n",
    "\n",
    "        user_text = extract_text(msgs[0][\"content\"])\n",
    "        assistant_text = extract_text(msgs[1][\"content\"])\n",
    "\n",
    "        content_user = []\n",
    "\n",
    "        # Only add image if it exists\n",
    "        if images:\n",
    "            image_path = os.path.join(base_path, images[0])\n",
    "            content_user.append({\"type\": \"image\", \"image\": image_path})\n",
    "\n",
    "        content_user.append({\"type\": \"text\", \"text\": user_text})\n",
    "\n",
    "        messages_out.append([\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": content_user,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": assistant_text}\n",
    "                ],\n",
    "            },\n",
    "        ])\n",
    "\n",
    "    return {\"messages\": messages_out}\n",
    "\n",
    "\n",
    "\n",
    "dataset = Dataset.from_list(normalized)\n",
    "dataset = dataset.map(\n",
    "    to_unsloth_vl,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "674c2084-a890-4dff-8858-b4210a375972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[{'content': [{'image': 'workspace/Dataset/images/1.jpeg', 'text': None, 'type': 'image'}, {'image': None, 'text': 'A 20-year-old woman from Sudan presents to a hospital in northern Uganda with a 2-day history of fever (39.6¬∞C), severe asthenia, chest and abdominal pain, nausea, vomiting, diarrhoea, and slight non-productive cough. Physical examination reveals semiconsciousness, hypotension (BP 90/60mmHg), abdominal tenderness, hepatosplenomegaly, and bleeding from the gums. Later, her condition rapidly worsens, and the clinical photograph shows copious oral bleeding (_page_1_Picture_4.jpeg). What is the most likely diagnosis, and what are the primary differential diagnoses that need to be considered in this epidemiological context?', 'type': 'text'}], 'role': 'user'}, {'content': [{'image': None, 'text': \"Given the patient's presentation with high fever (39.6¬∞C), shock (BP 90/60mmHg), multiple systemic symptoms, hepatosplenomegaly, and significant haemorrhage (gum bleeding progressing to copious oral bleeding), particularly during an Ebola virus disease outbreak in northern Uganda, a viral haemorrhagic fever (VHF) syndrome is the most likely diagnosis. The rapid deterioration and severe bleeding, as depicted in the image, strongly point towards a filoviral infection like Ebola. However, in this region, it is crucial to consider other primary differential diagnoses with similar haemorrhagic potential, including complicated malaria, typhoid fever, bacterial gastroenteritis, and leptospirosis, all of which can present with fever and bleeding manifestations.\", 'type': 'text'}], 'role': 'assistant'}]\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[0]\n",
    "\n",
    "print(type(sample[\"messages\"]))\n",
    "print(sample[\"messages\"])\n",
    "print(type(sample[\"messages\"][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fda0db47-fee9-4bb1-85ee-d071a9a3f06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Model does not have a default image size - using 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[accelerate.utils.other|WARNING]Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from unsloth.trainer import UnslothVisionDataCollator\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "FastVisionModel.for_training(model) # Enable for training!\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = UnslothVisionDataCollator(model, tokenizer), # Must use!\n",
    "    train_dataset = dataset,\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        #max_steps = 30,\n",
    "        num_train_epochs = 3, # Set this instead of max_steps for full training runs\n",
    "        learning_rate = 2e-4,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\",     # For Weights and Biases\n",
    "\n",
    "        # You MUST put the below items for vision finetuning:\n",
    "        remove_unused_columns = False,\n",
    "        dataset_text_field = \"\",\n",
    "        dataset_kwargs = {\"skip_prepare_dataset\": True},\n",
    "        max_length = 2048,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39285fc6-d64d-40b9-a703-aa4ed7863234",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 395 | Num Epochs = 3 | Total steps = 150\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 25,427,968 of 8,316,803,584 (0.31% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 05:26, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.087900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.032200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.802800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.935300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.829500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.863400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.743700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.758000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.602500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.624100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.450700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.585300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.515300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.430300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.306300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.442900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.381600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.383400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.399300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.476200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.521200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.457600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.296500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.319300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.411700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.418700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.275600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.435100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.355900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.548000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.402900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.406100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.316900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.361700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.402400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.415600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.213500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.322300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.316500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.432500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.225400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.374000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.402600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.381600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.307200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.258100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.338200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.368700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.270800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.176400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.132600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.263600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.164400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.177300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.142500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.209800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.194800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.174100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.160900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>1.078600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.255100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>1.205800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.147500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.303100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>1.226100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>1.134200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>1.151000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>1.117300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.231300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>1.110100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>1.007300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>1.075300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>1.071100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.251600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>1.108800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>1.082500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>1.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>1.163500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.217600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>1.227400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>1.069700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>1.177000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>1.027900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>1.129000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>1.052200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>1.053200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>1.080800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>1.110100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.023200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>1.166100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>1.198600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>1.147900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>1.011100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>1.080700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>1.128400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>1.049800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>1.092400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>1.116600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.064600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>1.054800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.949400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.950700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.966800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.849600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.900300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.945400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.876700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.968000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>1.074800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.870000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.824000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.929100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.842000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.915300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>1.012600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.998300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.897100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.995500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.869700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.831600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.891600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.858100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.937900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.833000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.812100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.848100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.948100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.983500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.985100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.925700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.847800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.818100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.791500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.878600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.940100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.845100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.942100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.881200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.759700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>0.872000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.790400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.863000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.912200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.885300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.873500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.760900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.819000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=150, training_loss=1.1702577741940816, metrics={'train_runtime': 338.7421, 'train_samples_per_second': 3.498, 'train_steps_per_second': 0.443, 'total_flos': 2.780876610121728e+16, 'train_loss': 1.1702577741940816, 'epoch': 3.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a96caf-7c57-42cb-9a3c-5fca2701db56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files (0 / 0): |          |  0.00B /  0.00B            \n",
      "Processing Files (1 / 1): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11.4MB / 11.4MB,  0.00B/s  \n",
      "New Data Upload: |          |  0.00B /  0.00B,  0.00B/s  \n"
     ]
    }
   ],
   "source": [
    "tokenizer.push_to_hub(\"usernameAvailable142/qwen2-vl-finetuned-lora\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2c9a8f-85a2-4b67-83a7-3cde9bf78bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files (0 / 0): |          |  0.00B /  0.00B            \n",
      "Processing Files (0 / 1):   1%|          |  559kB /  102MB,  699kB/s  \n",
      "Processing Files (0 / 1):   2%|‚ñè         | 1.68MB /  102MB, 1.68MB/s  \n",
      "Processing Files (0 / 1):   3%|‚ñé         | 2.79MB /  102MB, 2.33MB/s  \n",
      "Processing Files (0 / 1):   6%|‚ñå         | 6.15MB /  102MB, 4.39MB/s  \n",
      "Processing Files (0 / 1):  10%|‚ñà         | 10.6MB /  102MB, 6.63MB/s  \n",
      "Processing Files (0 / 1):  17%|‚ñà‚ñã        | 17.3MB /  102MB, 9.62MB/s  \n",
      "Processing Files (0 / 1):  18%|‚ñà‚ñä        | 18.4MB /  102MB, 9.21MB/s  \n",
      "Processing Files (0 / 1):  22%|‚ñà‚ñà‚ñè       | 22.9MB /  102MB, 10.4MB/s  \n",
      "Processing Files (0 / 1):  30%|‚ñà‚ñà‚ñâ       | 30.2MB /  102MB, 12.6MB/s  \n",
      "Processing Files (0 / 1):  32%|‚ñà‚ñà‚ñà‚ñè      | 33.0MB /  102MB, 12.7MB/s  \n",
      "Processing Files (0 / 1):  37%|‚ñà‚ñà‚ñà‚ñã      | 37.4MB /  102MB, 13.4MB/s  \n",
      "Processing Files (0 / 1):  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42.4MB /  102MB, 14.1MB/s  \n",
      "Processing Files (0 / 1):  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 46.9MB /  102MB, 14.7MB/s  \n",
      "Processing Files (0 / 1):  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 52.0MB /  102MB, 15.3MB/s  \n",
      "Processing Files (0 / 1):  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 56.4MB /  102MB, 15.7MB/s  \n",
      "Processing Files (0 / 1):  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 61.4MB /  102MB, 16.2MB/s  \n",
      "Processing Files (0 / 1):  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 64.2MB /  102MB, 16.1MB/s  \n",
      "Processing Files (0 / 1):  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 69.8MB /  102MB, 16.6MB/s  \n",
      "Processing Files (0 / 1):  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 74.9MB /  102MB, 17.0MB/s  \n",
      "Processing Files (0 / 1):  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 79.3MB /  102MB, 17.2MB/s  \n",
      "Processing Files (0 / 1):  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 84.4MB /  102MB, 17.6MB/s  \n",
      "Processing Files (0 / 1):  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 89.9MB /  102MB, 18.0MB/s  \n",
      "Processing Files (0 / 1):  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 93.8MB /  102MB, 18.0MB/s  \n",
      "Processing Files (0 / 1):  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 96.1MB /  102MB, 17.2MB/s  \n",
      "Processing Files (0 / 1):  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 97.2MB /  102MB, 16.2MB/s  \n",
      "Processing Files (0 / 1):  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 98.9MB /  102MB, 15.9MB/s  \n",
      "Processing Files (0 / 1):  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 99.3MB /  102MB, 15.5MB/s  \n",
      "Processing Files (0 / 1):  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ|  101MB /  102MB, 15.3MB/s  \n",
      "Processing Files (0 / 1): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ|  102MB /  102MB, 14.9MB/s  \n",
      "Processing Files (1 / 1): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|  102MB /  102MB, 12.7MB/s  \n",
      "Processing Files (1 / 1): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|  102MB /  102MB, 12.1MB/s  \n",
      "New Data Upload: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|  102MB /  102MB, 12.1MB/s  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to https://huggingface.co/usernameAvailable142/qwen2-vl-finetuned-lora\n"
     ]
    }
   ],
   "source": [
    "model.push_to_hub(\"usernameAvailable142/qwen2-vl-finetuned-lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa927a0-71d7-45d0-97c0-4a69c295d55d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Unsloth (RTX 5090)",
   "language": "python",
   "name": "unsloth_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
